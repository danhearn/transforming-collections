# Semantic Sound Documentation

This section discusses the methodology behing the semantic sound data sonification schemata, implemented in the pure data patches (link the pure data patches when they're uploaded). There are 2 separate channels: museum and archive. 

## Museum

Trained a Machine Learning (ML) algorithm on field recordings recorded within archival spaces at the Tate Modern. These sounds were fed into a [Realtime Audio Variational autoEncoder (RAVE)](https://github.com/acids-ircam/RAVE) develop by IRCAM - this neural network learns to re-synthesise the sounds, artificially, in real-time. The model learns a compressed, low-dimensional representation of the high-dimensional audio input. This compressed "manifold" can be explored through exploration of the "latent space" - movement within this space will modify the audio output, corresponding to different learned representations of the archival training data. For example, one region within the latent space could correspond to the background chatter of voices. This latent space is explored, and semantically meaningful movements can be automated.

The Museum model takes a 5-dimensional input corresponding to the vectorised key words tied to the current work. This vector modulates spatial position in the latent space - this modulation is aleatoric in nature but constrained by the semantically meaningful regions recorded in the exploration phase. This creates a generative soundscape, constantly changing, yet supporting the conceptual framework of the artwork. Through this methodology, archival sonification feeds back into the museum collection. 